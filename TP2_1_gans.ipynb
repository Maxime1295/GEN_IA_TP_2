{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Practical Exercise: GAN on Lego Brick Images**\n",
    "\n",
    "### **Goal**\n",
    "\n",
    "Train a DCGAN (Deep Convolutional GAN) to generate **64×64** grayscale images of **Lego bricks**.\n",
    "\n",
    "## **1\\. Understanding the Dataset**\n",
    "\n",
    "* **Data**: A Kaggle dataset containing many **png/jpg** images of lego bricks.  \n",
    "* **Tasks**:  \n",
    "  1. **Load** all images from a folder (since you only need images, no labels required).  \n",
    "  2. **Transform** images:  \n",
    "     * Convert to **grayscale** (one channel).  \n",
    "     * **Resize** to 64×64.  \n",
    "     * **Normalize** the pixel range, usually to \\[−1,1\\]\\[−1,1\\] or \\[0,1\\]\\[0,1\\]. Which one should you  pick here and why ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install kaggle\n",
    "!kaggle datasets download -d joosthazelzet/lego-brick-images --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------\n",
    "#   Hyperparameters\n",
    "# --------------------\n",
    "IMAGE_SIZE = 64       # Input/Output resolution\n",
    "CHANNELS = 1          # Grayscale = 1, for color use 3\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 100           # Dim of latent vector\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.0002\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "DEVICE = \"mps\"  # or \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "NOISE_PARAM = 0.1     # Label smoothing/noise\n",
    "# --------------------\n",
    "#   Transforms\n",
    "# --------------------\n",
    "# Define the transforms: Grayscale + Resize + Normalize\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(1), # Grayscale\n",
    "    transforms.Resize((64,64)), # Resiz\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)) # Center around 0 with range [-1,1]\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3\\. Build Dataset & DataLoader  \n",
    "Simple custom dataset that returns images (no labels) from a specified folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------\n",
    "# Dataset & DataLoader\n",
    "#--------------------\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Gather all image files with .png, .jpg, .jpeg\n",
    "        self.image_files = sorted([\n",
    "            f for f in os.listdir(root_dir) \n",
    "            if f.endswith(('.png', '.jpg', '.jpeg'))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # or \"L\" for grayscale\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "# Specify your dataset folder path\n",
    "dataset_path = \"dataset\"\n",
    "dataset = ImageDataset(root_dir=dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4\\. Plot few example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_images(loader):\n",
    "    xxx\n",
    "plot_sample_images(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2\\. Building the GAN Architecture**\n",
    "\n",
    "### **2.1 Discriminator**\n",
    "\n",
    "* **Input**: A 64×64 image (real or generated).  \n",
    "* **Goal**: Output a **single value** (e.g., the probability of the image being real).  \n",
    "* **Layers**: Use `Conv2d` to downsample from 64×64 to 1×1.  \n",
    "* **Activation**:  \n",
    "  * **LeakyReLU** after each convolution.  \n",
    "  * **Sigmoid** in the final layer for a probability output.  \n",
    "* **BatchNorm**: Commonly used after intermediate layers for more stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "#   Discriminator\n",
    "# ------------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            \n",
    "            # Input shape: (CHANNELS, 64, 64)\n",
    "            nn.Conv2d(CHANNELS, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True), # activation function \n",
    "            nn.Dropout(0.3), #dropout\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),# conv\n",
    "            nn.BatchNorm2d(128), #batch norm\n",
    "            nn.LeakyReLU(0.2, inplace=True), # activation function \n",
    "            nn.Dropout(0.3), #dropout\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3), # dropout\n",
    "\n",
    "            # Output shape: (1, 1, 1) -> Flatten to scalar\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid(),# activation function [0,1]\n",
    "            nn.Flatten(start_dim=1) # Flatten shape: (batch_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Generator**\n",
    "\n",
    "* **Input**: A random noise vector z of dimension ZdimZdim​ (e.g., 100).  \n",
    "* **Goal**: Generate a 64×64 grayscale image from random noise.  \n",
    "* **Layers**: Use `ConvTranspose2d` (also called **deconvolution**) to upsample the noise from 1×1 to 64×64.  \n",
    "* **Activation**:  \n",
    "  * Typically **LeakyReLU** after each layer (except the last).  \n",
    "  * Final layer often uses **Tanh** (so outputs are in \\[−1,1\\]\\[−1,1\\] range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------\n",
    "#   Generator\n",
    "# ------------------------------\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (Z_DIM, 1, 1)\n",
    "            nn.ConvTranspose2d(Z_DIM, xxx, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(xxx), #norm\n",
    "            nn.LeakyReLU(0.2, inplace=True), #activatio\n",
    "\n",
    "            nn.ConvTranspose2d(xxx, xxx, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(xxx),\n",
    "            nn.xxx(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(8, 16, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(xxx),\n",
    "            nn.xxx(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 32, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.xxx(0.2, inplace=True),\n",
    "\n",
    "            nn.xxx(32, CHANNELS, 4, 2, 1, bias=False),\n",
    "            nn.xxx() # activation function [-1,1]\n",
    "            # Output shape: (CHANNELS, 64, 64)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate models\n",
    "critic = Discriminator().to(DEVICE)\n",
    "generator = Generator().to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3\\. Training Setup**\n",
    "\n",
    "1. **Loss Functions**:  \n",
    "   * Use **Binary Cross-Entropy (BCE) Loss** for both generator and discriminator.  \n",
    "   * **Real images** → label \\= 1  \n",
    "   * **Fake images** → label \\= 0  \n",
    "2. **Optimizers**:  \n",
    "   * Typically **Adam** with `betas=(0.5, 0.999)`.  \n",
    "3. **Noise Labels** (Optional trick):  \n",
    "   * Sometimes add **label noise**: slightly randomizing real/fake labels or values near 1/0.  \n",
    "   * This can help prevent the discriminator from overpowering the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------\n",
    "#  Losses & Optimizers\n",
    "#------------------------\n",
    "criterion = nn.xxx()  # Binary Cross Entropy\n",
    "d_optimizer = xxx.xxx(xxx.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "g_optimizer = optim.Adam(xxx.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "#------------------------\n",
    "# Training\n",
    "#------------------------\n",
    "\n",
    "g_loss_values = []\n",
    "d_loss_values = []\n",
    "g_acc_values = []\n",
    "d_acc_values = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    g_loss_batch = []\n",
    "    d_loss_batch = []\n",
    "    g_acc_batch = []\n",
    "    d_acc_batch = []\n",
    "\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for i, real_images in progress_bar:\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # 1) Train Discriminator\n",
    "        xxx.xxx()\n",
    "\n",
    "        # True labels near 1, with some noise\n",
    "        real_labels = torch.xxx(xxx, 1, device=DEVICE)\n",
    "        real_noisy_labels = real_labels - NOISE_PARAM * torch.rand_like(real_labels) \n",
    "        # Fake labels near 0, with some noise\n",
    "        fake_labels = torch.xxx(xxx, 1, device=DEVICE)\n",
    "        fake_noisy_labels = fake_labels + NOISE_PARAM * torch.rand_like(fake_labels)\n",
    "\n",
    "        # Real images\n",
    "        real_preds = xxx(real_images)\n",
    "        real_loss = xxx(real_preds, real_noisy_labels) #get loss\n",
    "\n",
    "        # Fake images\n",
    "        z = torch.randn(batch_size, Z_DIM, 1, 1, device=DEVICE)\n",
    "        fake_images = xxx(z)\n",
    "        fake_preds = xxx(fake_images.detach())\n",
    "        fake_loss = xxx(fake_preds, fake_noisy_labels)\n",
    "\n",
    "        d_loss = (xxx + xxx) / 2\n",
    "        d_loss.xxx()\n",
    "        d_optimizer.xxx()\n",
    "\n",
    "        # Compute discriminator accuracy\n",
    "        d_real_acc = (real_preds.round() == real_labels).float().mean().item()\n",
    "        d_fake_acc = (fake_preds.round() == fake_labels).float().mean().item()\n",
    "        d_acc = (d_real_acc + d_fake_acc) / 2\n",
    "\n",
    "        # 2) Train Generator\n",
    "        xxx.xxx()\n",
    "        fake_preds = xxx(fake_images)  # do not detach here\n",
    "        # Generator wants D to say \"real\" => label=1\n",
    "        g_loss = xxx(fake_preds, real_labels)\n",
    "        xxx.xxx()\n",
    "        xxx.xxx()\n",
    "\n",
    "        g_acc = (fake_preds.round() == real_labels).float().mean().item()\n",
    "\n",
    "        # Track batch losses & accuracy\n",
    "        d_loss_batch.append(d_loss.item())\n",
    "        g_loss_batch.append(g_loss.item())\n",
    "        d_acc_batch.append(d_acc)\n",
    "        g_acc_batch.append(g_acc)\n",
    "\n",
    "        progress_bar.set_postfix(D_Loss=d_loss.item(), G_Loss=g_loss.item())\n",
    "\n",
    "    # End of epoch: record average metrics\n",
    "    d_loss_values.append(np.mean(d_loss_batch))\n",
    "    g_loss_values.append(np.mean(g_loss_batch))\n",
    "    d_acc_values.append(np.mean(d_acc_batch))\n",
    "    g_acc_values.append(np.mean(g_acc_batch))\n",
    "\n",
    "    # Occasionally save sample images\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        # Generate sample images\n",
    "        with torch.no_grad():\n",
    "            z_sample = torch.randn(16, Z_DIM, 1, 1, device=DEVICE)\n",
    "            fake_sample = generator(z_sample).cpu()\n",
    "        # Save or display using torchvision.utils\n",
    "        vutils.save_image(fake_sample, f\"fake_sample_epoch_{epoch+1:03d}.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If dose work then you can use a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load state dictionaries\n",
    "generator.load_state_dict(torch.load(GEN_PATH, map_location=DEVICE))\n",
    "critic.load_state_dict(torch.load(CRITIC_PATH, map_location=DEVICE))\n",
    "generator.eval()\n",
    "critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **5\\. Monitoring Progress**\n",
    "\n",
    "* **Loss Curves**: Plot **discriminator loss** and **generator loss**.  \n",
    "* **Accuracy Curves** (optional): Evaluate how often the discriminator is correct.  \n",
    "* **Save / Visualize** generated images periodically:  \n",
    "  * Sample random noise, generate images.  \n",
    "  * Compare how they evolve across epochs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "#  Plotting Results\n",
    "#--------------------------\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "titles = ['epoch_g_loss', 'epoch_g_acc', 'epoch_d_loss', 'epoch_d_acc']\n",
    "data = [g_loss_values, g_acc_values, d_loss_values, d_acc_values]\n",
    "\n",
    "for ax, title, values in zip(axes, titles, data):\n",
    "    ax.plot(values, color='brown')\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- What do you observe ? What's wrong ?   \n",
    "- If the **discriminator** loss is near zero but the **generator** loss is large, what does that imply?  \n",
    "- What does “mode collapse” look like?\n",
    "\n",
    "\n",
    "![][image1]\n",
    "\n",
    "## **6\\. Generating and Evaluating**\n",
    "\n",
    "1. **Sample** random zz vectors, generate images.  \n",
    "2. **Save** them as `.png` files or use a grid for visualization.  \n",
    "3. (Optional) Compare to **real dataset** images to see how **diverse** the outputs are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------\n",
    "#  Sampling\n",
    "#--------------------------\n",
    "# Generate a grid of images from random noise\n",
    "grid_width, grid_height = (8, 4)\n",
    "z_sample = torch.xxx(xxx * xxx, xxx, 1, 1, device=DEVICE)\n",
    "gen_imgs = xxx(z_sample).cpu().detach()\n",
    "\n",
    "# Show them in a matplotlib grid\n",
    "fig = plt.figure(figsize=(grid_width, grid_height))\n",
    "for i in range(grid_width * grid_height):\n",
    "    ax = fig.add_subplot(grid_height, grid_width, i + 1)\n",
    "    ax.axis(\"off\")\n",
    "    # Each image shape: (1, 64, 64)\n",
    "    ax.imshow(gen_imgs[i, 0, :, :], cmap=\"gray\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
