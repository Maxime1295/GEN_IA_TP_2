{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each practical exercise (TP), please work in groups of two or three. Then, create a **private GitHub repository** and add me (my GitHub is **arthur-75**) to your project. Finally, share the link to your project (or TP) under  [Practical Exercises](https://docs.google.com/spreadsheets/d/1V-YKgHn71FnwjoFltDhWsPJS7uIuAh9lj6SP2DSCvlY/edit?usp=sharing) and make sure to choose your **team name** :-)\n",
    "\n",
    "# **Conditional WGAN-GP on CelebA**\n",
    "\n",
    "## **1\\. Conditional GANs**\n",
    "\n",
    "A **Conditional GAN** is a variant of GANs where we **condition** both the **generator** and **critic** (discriminator) on **label information**. Instead of generating arbitrary images, we can **instruct** the GAN to generate images from a specific class or with a specific attribute (e.g., “Blond\\_Hair” \\= 0 or 1).\n",
    "\n",
    "### **What’s Different from a Standard GAN?**\n",
    "\n",
    "* You pass the **label** into both the **generator** and the **critic**.  \n",
    "* The generator learns to produce images that **match** that label.  \n",
    "* The critic can check if an image **matches** the label (i.e., it sees both the image and the label together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "#  HYPERPARAMETERS\n",
    "# -----------------------------\n",
    "IMAGE_SIZE = 64       # Image height/width\n",
    "CHANNELS = 3          # Using color images\n",
    "LABEL_DIM = 2         # e.g. \"Blond_Hair\" => 2 classes: 0 or 1\n",
    "BATCH_SIZE = 128\n",
    "Z_DIM = 32            # Dimension of noise\n",
    "LEARNING_RATE = 0.00005\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "EPOCHS = 20\n",
    "CRITIC_STEPS = 3      # Number of critic iterations per generator iteration\n",
    "GP_WEIGHT = 10.0      # Gradient penalty lambda\n",
    "\n",
    "DEVICE = \"mps\"#torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **2\\. Prepare the Dataset**\n",
    "\n",
    "1. **Collect** your images (e.g., CelebA).  \n",
    "2. **Resize** them to a consistent size (e.g. 64×64).  \n",
    "3. **Normalize** them to \\[−1,1\\] if you use **tanh** in the generator.  \n",
    "4. For **each** image, figure out its **label** (e.g., “Blond\\_Hair” \\= 0 or 1).  \n",
    "5. In your DataLoader, return **(image, label)** pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CSV that contains attributes (e.g., 'Blond_Hair')\n",
    "ATTR_CSV = \"xxx/list_attr_celeba.csv\"\n",
    "df = pd.read_csv(ATTR_CSV)\n",
    "\n",
    "# Suppose you want the attribute 'Blond_Hair' as a binary label: 0 or 1\n",
    "LABEL = \"Blond_Hair\" or something else (check the dataset)\n",
    "\n",
    "# Convert from [-1, +1] to [0, 1]\n",
    "df[LABEL] = df[LABEL].apply(lambda x: 1 if x == 1 else 0)\n",
    "\n",
    "# 'image_id' column has filenames like \"000001.jpg\", \"000002.jpg\", etc.\n",
    "img_fnames = df[\"image_id\"].tolist()\n",
    "labels = df[LABEL].tolist()\n",
    "\n",
    "# For example, confirm the first 5 filenames & labels\n",
    "print(img_fnames[:5])\n",
    "print(labels[:5])\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#   DATASET (Conditional)\n",
    "# -----------------------------\n",
    "class CelebAConditionalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A simple dataset that returns (image, label) pairs.\n",
    "    'label' is expected to be 0 or 1 for the chosen attribute.\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, img_fnames, labels, transform=None):\n",
    "        \"\"\"\n",
    "        root_dir: Folder with images\n",
    "        img_fnames: List of image file names\n",
    "        labels: List of 0/1 labels\n",
    "        transform: Torchvision transforms\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.img_fnames = img_fnames\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.img_fnames[idx])\n",
    "        image = torchvision.io.read_image(img_path)  # shape: (C, H, W), 0-255\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # e.g. resizing, normalization\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Example transform: resize, convert to [-1,1]\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ConvertImageDtype(torch.float),\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5)),\n",
    "])\n",
    "\n",
    "root_dir = \"xxx/img_align_celeba/img_align_celeba\"  # TODO: Change to your actual path\n",
    "dataset = xxx\n",
    "dataloader = xxx(xxx, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **3\\. Conditional Critic (Discriminator)**\n",
    "\n",
    "1. **Concatenate** the label with the image. For example:  \n",
    "   * If the image is (3,64,64)(RGB) and label is \\[0,1\\] replicate label across 64×64 to get a shape (2,64,64).  \n",
    "   * Then **cat** them along the channel dimension → (3+2,64,64).  \n",
    "2. **Downsample** this combined input with convolution layers to get a **single scalar output**.  \n",
    "3. **No Sigmoid** is required if using **Wasserstein** approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#   CONDITIONAL CRITIC (Discriminator)\n",
    "# -----------------------------\n",
    "class ConditionalCritic(nn.Module):\n",
    "    \"\"\"\n",
    "    Critic for Conditional WGAN-GP.\n",
    "    - Takes (image, label) pairs.\n",
    "    - Labels are expanded to match image spatial dimensions and concatenated to input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, img_channels=3, label_dim=2, base_features=64):\n",
    "        super().__init__()\n",
    "        self.img_channels = img_channels\n",
    "        self.label_dim = label_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (img_channels=3 + label_dim=2, base_features=64)\n",
    "            nn.Conv2d(xxx + xxx, xxx, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(base_features, base_features * 2, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(base_features * 2, base_features * 4, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Conv2d(base_features * 4, base_features * 8, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Final output: (B, 1, 1, 1) → Flatten to (B, 1)\n",
    "            nn.Conv2d(base_features * 8, 1, kernel_size=4, stride=1, padding=0),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, image, label):\n",
    "        \"\"\"\n",
    "        image: (B, 3, 64, 64)\n",
    "        label: (B,) or (B,1) or (B,2)\n",
    "        - Ensure label is one-hot: shape (B, label_dim).\n",
    "        - Expand to (B, label_dim, 64, 64) and concatenate with image.\n",
    "        \"\"\"\n",
    "        if label.dim() == 1 or label.shape[1] == 1:  # If not one-hot\n",
    "            label = F.one_hot(label.long(), num_classes= self.label_dim ).float()\n",
    "\n",
    "        # Expand label across spatial dimensions\n",
    "        bsz = label.size(0)\n",
    "        label_img = label.view(bsz, self.label_dim, 1, 1).repeat(1, 1, IMAGE_SIZE, IMAGE_SIZE)  # (B, label_dim, 64, 64)\n",
    "\n",
    "        concat_input = torch.cat([image, label_img], dim=1)  # (B, img_channels + label_dim, 64, 64)\n",
    "        return self.model(concat_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **4\\. Conditional Generator**\n",
    "\n",
    "1. **Concatenate** the **noise vector** (z) with the label.  \n",
    "2. Reshape to (z\\_dim  \\+ label\\_dim,1,1).  \n",
    "3. Use `ConvTranspose2d` layers (also called **deconvolution**) to **upsample** from (z\\_dim+label\\_dim,1,1) to (3,64,64).  \n",
    "4. Output activation \\= **tanh** to produce images in \\[−1,1\\].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#  CONDITIONAL GENERATOR\n",
    "# -----------------------------\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    Generator for Conditional WGAN-GP.\n",
    "    - Concatenates noise vector z and one-hot label.\n",
    "    - Reshapes input to (B, z_dim + label_dim, 1,1).\n",
    "    - Uses ConvTranspose2d layers to upsample to (B, 3, 64, 64).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, z_dim=32, label_dim=2, img_channels=3, base_features=64):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.label_dim = label_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            # Input: (z_dim + label_dim, 1, 1)\n",
    "            nn.ConvTranspose2d(xxx + xxx, base_features * 8, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(base_features * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(base_features * 8, base_features * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base_features * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(base_features * 4, base_features * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base_features * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(base_features * 2, base_features, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(base_features),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(base_features, img_channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.xxx()  # Output range: [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, label):\n",
    "        \"\"\"\n",
    "        noise: (B, z_dim, 1, 1)\n",
    "        label: (B,) or (B,1) or (B,2)\n",
    "        - Convert label to one-hot if necessary.\n",
    "        - Reshape label to (B, label_dim, 1,1) and concatenate with noise.\n",
    "        \"\"\"\n",
    "        if label.dim() == 1 or label.shape[1] == 1:  # If not one-hot\n",
    "            label = F.one_hot(label.long(), num_classes=self.label_dim).float()\n",
    "\n",
    "        bsz = label.size(0)\n",
    "        label_img = label.view(bsz, self.label_dim, 1, 1)  # (B, label_dim, 1,1)\n",
    "\n",
    "        z_cat = torch.cat([noise, label_img], dim=1)  # (B, z_dim + label_dim, 1,1)\n",
    "        return self.model(z_cat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **5\\. Conditioning in Both Networks**\n",
    "\n",
    "* **Critic** sees:  \n",
    "  * (image,label)  →  critique score  \n",
    "* **Generator** sees:  \n",
    "  * (noise,label)  →  image\n",
    "\n",
    "This ensures the generator **knows** which label it’s trying to produce, and the critic **checks** if the image is **real** *and* **matches** the label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#   INITIALIZE MODELS\n",
    "# -----------------------------\n",
    "critic = ConditionalCritic(img_channels=CHANNELS, label_dim=LABEL_DIM, base_features=64).to(DEVICE)\n",
    "generator = ConditionalGenerator(z_dim=Z_DIM, label_dim=LABEL_DIM, img_channels=CHANNELS, base_features=64).to(DEVICE)\n",
    "\n",
    "print(\"Conditional Critic and Generator successfully initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **6\\. Loss & Training (WGAN-GP Style)**\n",
    "\n",
    "1. **Critic Loss**:\n",
    "\n",
    "   * Loss \\= E\\[critic(fake,label)\\]  −  E\\[critic(real,label)\\]  +  λ⋅GP\n",
    "\n",
    "   * Where **GP** \\= Gradient Penalty (regularizing the critic’s gradient norm).\n",
    "\n",
    "2. **Generator Loss**:\n",
    "\n",
    "   * loss= − E\\[critic(fake,label)\\]\n",
    "\n",
    "   * Encourages the generator to produce images that the critic scores as **highly real**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------------\n",
    "#   OPTIMIZERS\n",
    "# -----------------------------\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "generator_optimizer = optim.Adam(generator.parameters(), lr=LEARNING_RATE, betas=(BETA1, BETA2))\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "#   HELPER FUNCTION: Gradient Penalty\n",
    "# -----------------------------\n",
    "def gradient_penalty(critic, real_imgs, fake_imgs, labels, device=DEVICE):\n",
    "    \"\"\"\n",
    "    Compute gradient penalty for WGAN-GP in a conditional setting:\n",
    "    1. Interpolate real & fake => interpolated\n",
    "    2. Critic(interpolated, label)\n",
    "    3. Penalty if norm-1 != 0\n",
    "    \"\"\"\n",
    "    bsz, C, H, W = real_imgs.shape\n",
    "    alpha = torch.rand(bsz, 1, 1, 1, device=device).expand(bsz, C, H, W)\n",
    "\n",
    "    interpolated = real_imgs * alpha + fake_imgs * (1 - alpha)\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    # Forward pass of critic\n",
    "    # We replicate label to match shape\n",
    "    critic_score = critic(interpolated, labels)\n",
    "\n",
    "    # Take gradient w.r.t. interpolated\n",
    "    grad = torch.autograd.grad(\n",
    "        inputs=interpolated,\n",
    "        outputs=critic_score,\n",
    "        grad_outputs=torch.ones_like(critic_score),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]  # shape => (bsz, C, H, W)\n",
    "\n",
    "    grad = grad.view(bsz, -1)  # flatten\n",
    "    grad_norm = grad.norm(2, dim=1)  # L2 norm per sample\n",
    "\n",
    "    gp = torch.mean((grad_norm - 1)**2)\n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **7\\. Training Loop**\n",
    "\n",
    "1. **For each epoch**:  \n",
    "   1. For each batch of (real\\_image,label)(real\\_image,label):  \n",
    "      * **Train Critic**:  \n",
    "        1. Generate fake images using (noise,label)(noise,label).  \n",
    "        2. Compute **critic loss** (real vs. fake, plus GP).  \n",
    "        3. **Backprop** \\+ **update** critic.  \n",
    "      * **Train Generator** (once after several critic updates):  \n",
    "        1. Generate new fake images.  \n",
    "        2. Compute **generator loss** (wants to fool critic).  \n",
    "        3. **Backprop** \\+ **update** generator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#   TRAINING LOOP\n",
    "# -----------------------------\n",
    "print(\"Starting conditional WGAN-GP training!\")\n",
    "g_losses, c_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    for batch_idx, (real_images, labels) in progress_bar:\n",
    "        real_images = real_images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        cur_bsz = real_images.size(0)\n",
    "\n",
    "        # -----------------------------\n",
    "        # (a) Train Critic multiple steps\n",
    "        # -----------------------------\n",
    "        for _ in range(CRITIC_STEPS):\n",
    "            noise = torch.randn(cur_bsz, Z_DIM, 1, 1, device=DEVICE)\n",
    "            fake_images = generator(noise, labels)\n",
    "\n",
    "            critic_optimizer.zero_grad()\n",
    "\n",
    "            critic_real = critic(real_images, labels).view(-1)\n",
    "            critic_fake = critic(fake_images, labels).view(-1)\n",
    "\n",
    "            wass_distance = (torch.mean(critic_fake) - torch.mean(critic_real))\n",
    "\n",
    "            gp = gradient_penalty(critic, real_imgs=real_images, fake_imgs=fake_images, labels=labels, device=DEVICE)\n",
    "            critic_loss = wass_distance + GP_WEIGHT*gp\n",
    "\n",
    "            critic_loss.backward(retain_graph=True)\n",
    "            critic_optimizer.step()\n",
    "\n",
    "        # -----------------------------\n",
    "        # (b) Train Generator\n",
    "        # -----------------------------\n",
    "        generator_optimizer.zero_grad()\n",
    "        critic_fake_for_g = critic(fake_images, labels).view(-1)\n",
    "        gen_loss = -torch.mean(critic_fake_for_g)\n",
    "        gen_loss.backward()\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        c_losses.append(critic_loss.item())\n",
    "        g_losses.append(gen_loss.item())\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            \"C_Loss\": f\"{critic_loss.item():.4f}\",\n",
    "            \"GP\": f\"{gp.item():.4f}\",\n",
    "            \"WassDist\": f\"{wass_distance.item():.4f}\",\n",
    "            \"G_Loss\": f\"{gen_loss.item():.4f}\"\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If dose work then you can use a pre-trained model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load state dictionaries\n",
    "generator.load_state_dict(torch.load(GEN_PATH, map_location=DEVICE))\n",
    "critic.load_state_dict(torch.load(CRITIC_PATH, map_location=DEVICE))\n",
    "generator.eval()\n",
    "critic.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **8\\. Monitoring Progress**\n",
    "\n",
    "1. **Plot** Critic Loss and Generator Loss over time.  \n",
    "2. Occasionally **sample** random noise (and label) → generate images.  \n",
    "3. See if the generated images match the label condition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "#   Plot losses\n",
    "# -----------------------------\n",
    "def plot_losses(g_losses, c_losses):\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.title(\"Generator & Critic Loss During Training (Cond WGAN-GP)\")\n",
    "    plt.plot(c_losses, label=\"Critic\")\n",
    "    plt.plot(g_losses, label=\"Generator\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_losses(g_losses, c_losses )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **9\\. Generating Images with a Chosen Label**\n",
    "\n",
    "* After training, pick a label (e.g., `label=1`).  \n",
    "* Sample noise z.  \n",
    "* Generate an image via Generator(z,label)Generator(z,label).  \n",
    "* Inspect if the attribute or class you chose is visible in the generated result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "#   Generate\n",
    "# -----------------------------\n",
    "def generate_samples(generator, label, num_samples=8):\n",
    "    \"\"\"\n",
    "    label: int or 0/1 (or a batch of them). \n",
    "    We'll produce 'num_samples' images for the specified label \n",
    "    and display them in a grid.\n",
    "    \"\"\"\n",
    "    generator.eval()\n",
    "    noise = torch.randn(num_samples, Z_DIM, 1, 1).to(DEVICE)\n",
    "\n",
    "    label_tensor = torch.tensor([label]*num_samples, device=DEVICE)\n",
    "    # If it's a 2-class attribute, we can do a one-hot if needed:\n",
    "    # label_tensor = F.one_hot(label_tensor, num_classes=2).float()\n",
    "    # For simplicity, assume we already are passing 0 or 1 => one-hot?\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = generator(noise, label_tensor)\n",
    "    fake_imgs = fake_imgs.cpu()*0.5 + 0.5\n",
    "\n",
    "    grid_img = torchvision.utils.make_grid(fake_imgs, nrow=4)\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.imshow(grid_img.permute(1,2,0).numpy())\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Generated (Label={label})\")\n",
    "    plt.show()\n",
    "    generator.train()\n",
    "\n",
    "# If you have a trained model:\n",
    "generate_samples(generator, label=1, num_samples=8)\n",
    "generate_samples(generator, label=0, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10\\. Improving the Model (optional)**\n",
    "\n",
    "* **Use More Classes**: Instead of 0/1, you can have multi-class labels (e.g., hair color with 4 or 5 categories).  \n",
    "* **Add More Convolutional Depth**: For sharper images, add more layers or bigger feature maps.  \n",
    "* **Change Image Size**: If you have enough GPU memory, use 128×128 or 256×256.  \n",
    "* **Try Different Normalization**: E.g., **Spectral Norm** in the critic.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
